{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           date                                               text president  \\\n",
       "0    2012-11-05  president obama tells the story of fired up re...     Obama   \n",
       "1    2012-11-06  election day is here confirm your polling plac...     Obama   \n",
       "2    2012-11-07  thank you president obama in his victory speec...     Obama   \n",
       "3    2012-11-08  the definition of hope is you still believe ev...     Obama   \n",
       "4    2012-11-09  what bobby kennedy called the ripples of hope ...     Obama   \n",
       "...         ...                                                ...       ...   \n",
       "1823 2020-06-11  our great national guard troops who took care ...     Trump   \n",
       "1824 2020-06-12  people have no idea how fake the lamestream me...     Trump   \n",
       "1825 2020-06-15  i’ve done more in less than 4 years than biden...     Trump   \n",
       "1826 2020-06-16  wow may retail sales show biggest one-month in...     Trump   \n",
       "1827 2020-06-17  joe biden was a total failure in government he...     Trump   \n",
       "\n",
       "      label  \n",
       "0         0  \n",
       "1         1  \n",
       "2         2  \n",
       "3         2  \n",
       "4         0  \n",
       "...     ...  \n",
       "1823      2  \n",
       "1824      2  \n",
       "1825      1  \n",
       "1826      0  \n",
       "1827      2  \n",
       "\n",
       "[1828 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>text</th>\n      <th>president</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2012-11-05</td>\n      <td>president obama tells the story of fired up re...</td>\n      <td>Obama</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2012-11-06</td>\n      <td>election day is here confirm your polling plac...</td>\n      <td>Obama</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2012-11-07</td>\n      <td>thank you president obama in his victory speec...</td>\n      <td>Obama</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2012-11-08</td>\n      <td>the definition of hope is you still believe ev...</td>\n      <td>Obama</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2012-11-09</td>\n      <td>what bobby kennedy called the ripples of hope ...</td>\n      <td>Obama</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1823</th>\n      <td>2020-06-11</td>\n      <td>our great national guard troops who took care ...</td>\n      <td>Trump</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1824</th>\n      <td>2020-06-12</td>\n      <td>people have no idea how fake the lamestream me...</td>\n      <td>Trump</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1825</th>\n      <td>2020-06-15</td>\n      <td>i’ve done more in less than 4 years than biden...</td>\n      <td>Trump</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1826</th>\n      <td>2020-06-16</td>\n      <td>wow may retail sales show biggest one-month in...</td>\n      <td>Trump</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1827</th>\n      <td>2020-06-17</td>\n      <td>joe biden was a total failure in government he...</td>\n      <td>Trump</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>1828 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_excel('../Adatok/tisztitott_adat.xlsx', header=0,index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4997854, 6534600)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "import gensim\n",
    "tokenized_tweet = data['text'].apply(lambda x: x.split())\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size=50, \n",
    "            window=5, \n",
    "            min_count=2,                                   \n",
    "            sg = 1,\n",
    "            hs = 0,\n",
    "            negative = 10, \n",
    "            workers= 32, \n",
    "            seed = 34\n",
    ")\n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(data['text']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('donald', 0.873864471912384),\n",
       " ('sabotage', 0.7372989654541016),\n",
       " ('j', 0.7238896489143372),\n",
       " ('media’s', 0.7036998271942139),\n",
       " ('campaign', 0.6925650835037231),\n",
       " ('foundation', 0.6880016922950745),\n",
       " ('trump’s', 0.6835409998893738),\n",
       " ('clinton', 0.676448404788971),\n",
       " ('i’d', 0.6709875464439392),\n",
       " ('scheme', 0.6678166389465332)]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(positive=\"trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:  # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1828, 50)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 50)) \n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 50)\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.176195  0.400107  0.063833 -0.047243  0.379398  0.001906 -0.240907   \n",
       "1     0.308915  0.387523 -0.073404 -0.127846  0.463324 -0.039386 -0.259344   \n",
       "2     0.276564  0.337364  0.037026 -0.077074  0.371610  0.086894 -0.138589   \n",
       "3     0.231620  0.360612 -0.085521 -0.126497  0.320687 -0.128666 -0.109749   \n",
       "4     0.189428  0.387376 -0.192068 -0.091749  0.427815 -0.127749 -0.082616   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1823  0.231262  0.357693 -0.189297 -0.123807  0.293092 -0.016057 -0.132901   \n",
       "1824  0.293496  0.337836 -0.118748 -0.110966  0.267164 -0.084375 -0.154838   \n",
       "1825  0.312147  0.410976 -0.177270 -0.093560  0.151374 -0.159984 -0.165689   \n",
       "1826  0.190224  0.292260 -0.244064 -0.202552  0.327836 -0.042354 -0.148880   \n",
       "1827  0.097581  0.258845 -0.033221 -0.091162  0.355591 -0.104697 -0.172699   \n",
       "\n",
       "             7         8         9  ...        41        42        43  \\\n",
       "0    -0.013293 -0.253030 -0.188061  ... -0.611825 -0.092432  0.026429   \n",
       "1    -0.122964 -0.301306 -0.129755  ... -0.615937 -0.036920  0.125880   \n",
       "2    -0.067244 -0.186002 -0.070232  ... -0.574181 -0.063649  0.189423   \n",
       "3    -0.170012 -0.286664 -0.001804  ... -0.711585 -0.130762  0.212588   \n",
       "4    -0.118818 -0.252675 -0.138974  ... -0.610949 -0.067066  0.220233   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1823 -0.048584 -0.239733  0.005645  ... -0.592727 -0.181990  0.205931   \n",
       "1824 -0.106967 -0.210433 -0.026474  ... -0.628223 -0.159698  0.185550   \n",
       "1825 -0.121895 -0.261683 -0.004379  ... -0.595919 -0.163216  0.128714   \n",
       "1826 -0.051078 -0.193633  0.066048  ... -0.587513 -0.212488  0.241919   \n",
       "1827 -0.032790 -0.407769  0.147575  ... -0.606461 -0.153643  0.166494   \n",
       "\n",
       "            44        45        46        47        48        49  label  \n",
       "0     0.117207 -0.073067  0.311092 -0.450759  0.145084 -0.355200      0  \n",
       "1    -0.037207 -0.190333  0.163252 -0.602922  0.143013 -0.436464      1  \n",
       "2    -0.030094 -0.066840  0.109191 -0.605567  0.172048 -0.408675      2  \n",
       "3     0.027735 -0.103147  0.099261 -0.483103  0.304043 -0.399509      2  \n",
       "4     0.000427 -0.137080  0.185548 -0.602420  0.219906 -0.541185      0  \n",
       "...        ...       ...       ...       ...       ...       ...    ...  \n",
       "1823 -0.030121 -0.198894  0.113111 -0.566411  0.008201 -0.389829      2  \n",
       "1824  0.017761 -0.151750  0.154438 -0.527447  0.047161 -0.401636      2  \n",
       "1825 -0.064329 -0.178822  0.122368 -0.468430  0.094918 -0.391055      1  \n",
       "1826  0.018754 -0.178455  0.085308 -0.580205 -0.022784 -0.385212      0  \n",
       "1827  0.057345 -0.335258  0.053288 -0.504627 -0.072240 -0.358602      2  \n",
       "\n",
       "[1828 rows x 51 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>41</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.176195</td>\n      <td>0.400107</td>\n      <td>0.063833</td>\n      <td>-0.047243</td>\n      <td>0.379398</td>\n      <td>0.001906</td>\n      <td>-0.240907</td>\n      <td>-0.013293</td>\n      <td>-0.253030</td>\n      <td>-0.188061</td>\n      <td>...</td>\n      <td>-0.611825</td>\n      <td>-0.092432</td>\n      <td>0.026429</td>\n      <td>0.117207</td>\n      <td>-0.073067</td>\n      <td>0.311092</td>\n      <td>-0.450759</td>\n      <td>0.145084</td>\n      <td>-0.355200</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.308915</td>\n      <td>0.387523</td>\n      <td>-0.073404</td>\n      <td>-0.127846</td>\n      <td>0.463324</td>\n      <td>-0.039386</td>\n      <td>-0.259344</td>\n      <td>-0.122964</td>\n      <td>-0.301306</td>\n      <td>-0.129755</td>\n      <td>...</td>\n      <td>-0.615937</td>\n      <td>-0.036920</td>\n      <td>0.125880</td>\n      <td>-0.037207</td>\n      <td>-0.190333</td>\n      <td>0.163252</td>\n      <td>-0.602922</td>\n      <td>0.143013</td>\n      <td>-0.436464</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.276564</td>\n      <td>0.337364</td>\n      <td>0.037026</td>\n      <td>-0.077074</td>\n      <td>0.371610</td>\n      <td>0.086894</td>\n      <td>-0.138589</td>\n      <td>-0.067244</td>\n      <td>-0.186002</td>\n      <td>-0.070232</td>\n      <td>...</td>\n      <td>-0.574181</td>\n      <td>-0.063649</td>\n      <td>0.189423</td>\n      <td>-0.030094</td>\n      <td>-0.066840</td>\n      <td>0.109191</td>\n      <td>-0.605567</td>\n      <td>0.172048</td>\n      <td>-0.408675</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.231620</td>\n      <td>0.360612</td>\n      <td>-0.085521</td>\n      <td>-0.126497</td>\n      <td>0.320687</td>\n      <td>-0.128666</td>\n      <td>-0.109749</td>\n      <td>-0.170012</td>\n      <td>-0.286664</td>\n      <td>-0.001804</td>\n      <td>...</td>\n      <td>-0.711585</td>\n      <td>-0.130762</td>\n      <td>0.212588</td>\n      <td>0.027735</td>\n      <td>-0.103147</td>\n      <td>0.099261</td>\n      <td>-0.483103</td>\n      <td>0.304043</td>\n      <td>-0.399509</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.189428</td>\n      <td>0.387376</td>\n      <td>-0.192068</td>\n      <td>-0.091749</td>\n      <td>0.427815</td>\n      <td>-0.127749</td>\n      <td>-0.082616</td>\n      <td>-0.118818</td>\n      <td>-0.252675</td>\n      <td>-0.138974</td>\n      <td>...</td>\n      <td>-0.610949</td>\n      <td>-0.067066</td>\n      <td>0.220233</td>\n      <td>0.000427</td>\n      <td>-0.137080</td>\n      <td>0.185548</td>\n      <td>-0.602420</td>\n      <td>0.219906</td>\n      <td>-0.541185</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1823</th>\n      <td>0.231262</td>\n      <td>0.357693</td>\n      <td>-0.189297</td>\n      <td>-0.123807</td>\n      <td>0.293092</td>\n      <td>-0.016057</td>\n      <td>-0.132901</td>\n      <td>-0.048584</td>\n      <td>-0.239733</td>\n      <td>0.005645</td>\n      <td>...</td>\n      <td>-0.592727</td>\n      <td>-0.181990</td>\n      <td>0.205931</td>\n      <td>-0.030121</td>\n      <td>-0.198894</td>\n      <td>0.113111</td>\n      <td>-0.566411</td>\n      <td>0.008201</td>\n      <td>-0.389829</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1824</th>\n      <td>0.293496</td>\n      <td>0.337836</td>\n      <td>-0.118748</td>\n      <td>-0.110966</td>\n      <td>0.267164</td>\n      <td>-0.084375</td>\n      <td>-0.154838</td>\n      <td>-0.106967</td>\n      <td>-0.210433</td>\n      <td>-0.026474</td>\n      <td>...</td>\n      <td>-0.628223</td>\n      <td>-0.159698</td>\n      <td>0.185550</td>\n      <td>0.017761</td>\n      <td>-0.151750</td>\n      <td>0.154438</td>\n      <td>-0.527447</td>\n      <td>0.047161</td>\n      <td>-0.401636</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1825</th>\n      <td>0.312147</td>\n      <td>0.410976</td>\n      <td>-0.177270</td>\n      <td>-0.093560</td>\n      <td>0.151374</td>\n      <td>-0.159984</td>\n      <td>-0.165689</td>\n      <td>-0.121895</td>\n      <td>-0.261683</td>\n      <td>-0.004379</td>\n      <td>...</td>\n      <td>-0.595919</td>\n      <td>-0.163216</td>\n      <td>0.128714</td>\n      <td>-0.064329</td>\n      <td>-0.178822</td>\n      <td>0.122368</td>\n      <td>-0.468430</td>\n      <td>0.094918</td>\n      <td>-0.391055</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1826</th>\n      <td>0.190224</td>\n      <td>0.292260</td>\n      <td>-0.244064</td>\n      <td>-0.202552</td>\n      <td>0.327836</td>\n      <td>-0.042354</td>\n      <td>-0.148880</td>\n      <td>-0.051078</td>\n      <td>-0.193633</td>\n      <td>0.066048</td>\n      <td>...</td>\n      <td>-0.587513</td>\n      <td>-0.212488</td>\n      <td>0.241919</td>\n      <td>0.018754</td>\n      <td>-0.178455</td>\n      <td>0.085308</td>\n      <td>-0.580205</td>\n      <td>-0.022784</td>\n      <td>-0.385212</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1827</th>\n      <td>0.097581</td>\n      <td>0.258845</td>\n      <td>-0.033221</td>\n      <td>-0.091162</td>\n      <td>0.355591</td>\n      <td>-0.104697</td>\n      <td>-0.172699</td>\n      <td>-0.032790</td>\n      <td>-0.407769</td>\n      <td>0.147575</td>\n      <td>...</td>\n      <td>-0.606461</td>\n      <td>-0.153643</td>\n      <td>0.166494</td>\n      <td>0.057345</td>\n      <td>-0.335258</td>\n      <td>0.053288</td>\n      <td>-0.504627</td>\n      <td>-0.072240</td>\n      <td>-0.358602</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>1828 rows × 51 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "vec_data = pd.concat([wordvec_df, data['label']], axis=1)\n",
    "vec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label = vec_data['label']\n",
    "attrs = vec_data.drop(columns=['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        attrs, label, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.5842450765864332\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.5645514223194749\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=30, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}