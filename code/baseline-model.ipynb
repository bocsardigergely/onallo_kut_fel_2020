{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           date                                               text president  \\\n",
       "0    2012-11-05  president obama tells the story of fired up re...     Obama   \n",
       "1    2012-11-06  election day is here confirm your polling plac...     Obama   \n",
       "2    2012-11-07  thank you president obama in his victory speec...     Obama   \n",
       "3    2012-11-08  the definition of hope is you still believe ev...     Obama   \n",
       "4    2012-11-09  what bobby kennedy called the ripples of hope ...     Obama   \n",
       "...         ...                                                ...       ...   \n",
       "1823 2020-06-11  our great national guard troops who took care ...     Trump   \n",
       "1824 2020-06-12  people have no idea how fake the lamestream me...     Trump   \n",
       "1825 2020-06-15  i’ve done more in less than 4 years than biden...     Trump   \n",
       "1826 2020-06-16  wow may retail sales show biggest one-month in...     Trump   \n",
       "1827 2020-06-17  joe biden was a total failure in government he...     Trump   \n",
       "\n",
       "      label  \n",
       "0         0  \n",
       "1         0  \n",
       "2        -1  \n",
       "3        -1  \n",
       "4         0  \n",
       "...     ...  \n",
       "1823     -1  \n",
       "1824     -1  \n",
       "1825      1  \n",
       "1826      0  \n",
       "1827     -1  \n",
       "\n",
       "[1828 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>text</th>\n      <th>president</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2012-11-05</td>\n      <td>president obama tells the story of fired up re...</td>\n      <td>Obama</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2012-11-06</td>\n      <td>election day is here confirm your polling plac...</td>\n      <td>Obama</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2012-11-07</td>\n      <td>thank you president obama in his victory speec...</td>\n      <td>Obama</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2012-11-08</td>\n      <td>the definition of hope is you still believe ev...</td>\n      <td>Obama</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2012-11-09</td>\n      <td>what bobby kennedy called the ripples of hope ...</td>\n      <td>Obama</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1823</th>\n      <td>2020-06-11</td>\n      <td>our great national guard troops who took care ...</td>\n      <td>Trump</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1824</th>\n      <td>2020-06-12</td>\n      <td>people have no idea how fake the lamestream me...</td>\n      <td>Trump</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1825</th>\n      <td>2020-06-15</td>\n      <td>i’ve done more in less than 4 years than biden...</td>\n      <td>Trump</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1826</th>\n      <td>2020-06-16</td>\n      <td>wow may retail sales show biggest one-month in...</td>\n      <td>Trump</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1827</th>\n      <td>2020-06-17</td>\n      <td>joe biden was a total failure in government he...</td>\n      <td>Trump</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1828 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_excel('../Adatok/tisztitott_adat.xlsx', header=0,index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4996349, 6534600)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "import gensim\n",
    "tokenized_tweet = data['text'].apply(lambda x: x.split())\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size=50, \n",
    "            window=5, \n",
    "            min_count=2,                                   \n",
    "            sg = 1,\n",
    "            hs = 0,\n",
    "            negative = 10, \n",
    "            workers= 32, \n",
    "            seed = 34\n",
    ")\n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(data['text']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('donald', 0.8813544511795044),\n",
       " ('j', 0.7670353651046753),\n",
       " ('sabotage', 0.7368723154067993),\n",
       " ('frame', 0.685495913028717),\n",
       " ('trump’s', 0.678617000579834),\n",
       " ('foundation', 0.6685938835144043),\n",
       " ('takedown', 0.6636621356010437),\n",
       " ('plot', 0.6620488166809082),\n",
       " ('clinton', 0.6554353833198547),\n",
       " ('campaign', 0.652974545955658)]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(positive=\"trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:  # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1828, 50)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 50)) \n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 50)\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.211237  0.511313  0.020558 -0.130269  0.412363 -0.173577 -0.257338   \n",
       "1     0.262321  0.376918 -0.034491 -0.048589  0.408996 -0.078180 -0.207245   \n",
       "2     0.152753  0.287909 -0.009970  0.064506  0.382174 -0.133902 -0.183040   \n",
       "3     0.227675  0.380395 -0.072408 -0.015062  0.385880 -0.161939 -0.154601   \n",
       "4     0.073455  0.378790 -0.124944  0.005942  0.456297 -0.183703 -0.186649   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1823  0.166577  0.251380 -0.186115 -0.059232  0.246968 -0.030453 -0.191882   \n",
       "1824  0.200662  0.301914 -0.110469 -0.019978  0.225036 -0.146430 -0.181285   \n",
       "1825  0.190328  0.335932 -0.265849 -0.018815  0.130668 -0.204347 -0.192384   \n",
       "1826  0.147867  0.225133 -0.196530 -0.110264  0.291163 -0.043916 -0.216164   \n",
       "1827  0.224165  0.316879 -0.250276 -0.133932  0.364940 -0.075787 -0.290183   \n",
       "\n",
       "             7         8         9  ...        41        42        43  \\\n",
       "0    -0.076809 -0.226040 -0.238499  ... -0.495704  0.215028 -0.025649   \n",
       "1    -0.173951 -0.330650 -0.089396  ... -0.628082  0.066327  0.050675   \n",
       "2    -0.086652 -0.282310 -0.067722  ... -0.547999  0.076656  0.099634   \n",
       "3    -0.138655 -0.209884  0.005553  ... -0.573807 -0.048522  0.115793   \n",
       "4    -0.081883 -0.223629 -0.137716  ... -0.587914 -0.095553  0.160431   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1823 -0.095117 -0.324323  0.017951  ... -0.568921 -0.097746  0.196562   \n",
       "1824 -0.159599 -0.255236 -0.038092  ... -0.605691 -0.084217  0.161910   \n",
       "1825 -0.190700 -0.292305  0.009898  ... -0.580780 -0.146579  0.154963   \n",
       "1826 -0.102982 -0.336679  0.075210  ... -0.520532 -0.126400  0.191995   \n",
       "1827 -0.035032 -0.510206  0.034875  ... -0.543020 -0.105807  0.171141   \n",
       "\n",
       "            44        45        46        47        48        49  label  \n",
       "0     0.002599 -0.141979  0.266143 -0.360443 -0.102130 -0.254246      0  \n",
       "1    -0.025971 -0.237459  0.233096 -0.577206 -0.005746 -0.302730      0  \n",
       "2    -0.085858 -0.172792  0.182504 -0.504729 -0.033667 -0.246986     -1  \n",
       "3     0.028717 -0.117772  0.163182 -0.463737  0.089421 -0.385457     -1  \n",
       "4    -0.090187 -0.168324  0.257795 -0.590201 -0.000375 -0.373713      0  \n",
       "...        ...       ...       ...       ...       ...       ...    ...  \n",
       "1823 -0.041305 -0.194173  0.195119 -0.572311 -0.118294 -0.318756     -1  \n",
       "1824 -0.016286 -0.156041  0.214333 -0.521417 -0.054399 -0.279514     -1  \n",
       "1825 -0.057078 -0.148703  0.213551 -0.481174  0.023114 -0.254860      1  \n",
       "1826  0.017389 -0.161118  0.190478 -0.570801 -0.136490 -0.325469      0  \n",
       "1827 -0.105441 -0.231421  0.049828 -0.427197 -0.152171 -0.296015     -1  \n",
       "\n",
       "[1828 rows x 51 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>41</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.211237</td>\n      <td>0.511313</td>\n      <td>0.020558</td>\n      <td>-0.130269</td>\n      <td>0.412363</td>\n      <td>-0.173577</td>\n      <td>-0.257338</td>\n      <td>-0.076809</td>\n      <td>-0.226040</td>\n      <td>-0.238499</td>\n      <td>...</td>\n      <td>-0.495704</td>\n      <td>0.215028</td>\n      <td>-0.025649</td>\n      <td>0.002599</td>\n      <td>-0.141979</td>\n      <td>0.266143</td>\n      <td>-0.360443</td>\n      <td>-0.102130</td>\n      <td>-0.254246</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.262321</td>\n      <td>0.376918</td>\n      <td>-0.034491</td>\n      <td>-0.048589</td>\n      <td>0.408996</td>\n      <td>-0.078180</td>\n      <td>-0.207245</td>\n      <td>-0.173951</td>\n      <td>-0.330650</td>\n      <td>-0.089396</td>\n      <td>...</td>\n      <td>-0.628082</td>\n      <td>0.066327</td>\n      <td>0.050675</td>\n      <td>-0.025971</td>\n      <td>-0.237459</td>\n      <td>0.233096</td>\n      <td>-0.577206</td>\n      <td>-0.005746</td>\n      <td>-0.302730</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.152753</td>\n      <td>0.287909</td>\n      <td>-0.009970</td>\n      <td>0.064506</td>\n      <td>0.382174</td>\n      <td>-0.133902</td>\n      <td>-0.183040</td>\n      <td>-0.086652</td>\n      <td>-0.282310</td>\n      <td>-0.067722</td>\n      <td>...</td>\n      <td>-0.547999</td>\n      <td>0.076656</td>\n      <td>0.099634</td>\n      <td>-0.085858</td>\n      <td>-0.172792</td>\n      <td>0.182504</td>\n      <td>-0.504729</td>\n      <td>-0.033667</td>\n      <td>-0.246986</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.227675</td>\n      <td>0.380395</td>\n      <td>-0.072408</td>\n      <td>-0.015062</td>\n      <td>0.385880</td>\n      <td>-0.161939</td>\n      <td>-0.154601</td>\n      <td>-0.138655</td>\n      <td>-0.209884</td>\n      <td>0.005553</td>\n      <td>...</td>\n      <td>-0.573807</td>\n      <td>-0.048522</td>\n      <td>0.115793</td>\n      <td>0.028717</td>\n      <td>-0.117772</td>\n      <td>0.163182</td>\n      <td>-0.463737</td>\n      <td>0.089421</td>\n      <td>-0.385457</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.073455</td>\n      <td>0.378790</td>\n      <td>-0.124944</td>\n      <td>0.005942</td>\n      <td>0.456297</td>\n      <td>-0.183703</td>\n      <td>-0.186649</td>\n      <td>-0.081883</td>\n      <td>-0.223629</td>\n      <td>-0.137716</td>\n      <td>...</td>\n      <td>-0.587914</td>\n      <td>-0.095553</td>\n      <td>0.160431</td>\n      <td>-0.090187</td>\n      <td>-0.168324</td>\n      <td>0.257795</td>\n      <td>-0.590201</td>\n      <td>-0.000375</td>\n      <td>-0.373713</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1823</th>\n      <td>0.166577</td>\n      <td>0.251380</td>\n      <td>-0.186115</td>\n      <td>-0.059232</td>\n      <td>0.246968</td>\n      <td>-0.030453</td>\n      <td>-0.191882</td>\n      <td>-0.095117</td>\n      <td>-0.324323</td>\n      <td>0.017951</td>\n      <td>...</td>\n      <td>-0.568921</td>\n      <td>-0.097746</td>\n      <td>0.196562</td>\n      <td>-0.041305</td>\n      <td>-0.194173</td>\n      <td>0.195119</td>\n      <td>-0.572311</td>\n      <td>-0.118294</td>\n      <td>-0.318756</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1824</th>\n      <td>0.200662</td>\n      <td>0.301914</td>\n      <td>-0.110469</td>\n      <td>-0.019978</td>\n      <td>0.225036</td>\n      <td>-0.146430</td>\n      <td>-0.181285</td>\n      <td>-0.159599</td>\n      <td>-0.255236</td>\n      <td>-0.038092</td>\n      <td>...</td>\n      <td>-0.605691</td>\n      <td>-0.084217</td>\n      <td>0.161910</td>\n      <td>-0.016286</td>\n      <td>-0.156041</td>\n      <td>0.214333</td>\n      <td>-0.521417</td>\n      <td>-0.054399</td>\n      <td>-0.279514</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1825</th>\n      <td>0.190328</td>\n      <td>0.335932</td>\n      <td>-0.265849</td>\n      <td>-0.018815</td>\n      <td>0.130668</td>\n      <td>-0.204347</td>\n      <td>-0.192384</td>\n      <td>-0.190700</td>\n      <td>-0.292305</td>\n      <td>0.009898</td>\n      <td>...</td>\n      <td>-0.580780</td>\n      <td>-0.146579</td>\n      <td>0.154963</td>\n      <td>-0.057078</td>\n      <td>-0.148703</td>\n      <td>0.213551</td>\n      <td>-0.481174</td>\n      <td>0.023114</td>\n      <td>-0.254860</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1826</th>\n      <td>0.147867</td>\n      <td>0.225133</td>\n      <td>-0.196530</td>\n      <td>-0.110264</td>\n      <td>0.291163</td>\n      <td>-0.043916</td>\n      <td>-0.216164</td>\n      <td>-0.102982</td>\n      <td>-0.336679</td>\n      <td>0.075210</td>\n      <td>...</td>\n      <td>-0.520532</td>\n      <td>-0.126400</td>\n      <td>0.191995</td>\n      <td>0.017389</td>\n      <td>-0.161118</td>\n      <td>0.190478</td>\n      <td>-0.570801</td>\n      <td>-0.136490</td>\n      <td>-0.325469</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1827</th>\n      <td>0.224165</td>\n      <td>0.316879</td>\n      <td>-0.250276</td>\n      <td>-0.133932</td>\n      <td>0.364940</td>\n      <td>-0.075787</td>\n      <td>-0.290183</td>\n      <td>-0.035032</td>\n      <td>-0.510206</td>\n      <td>0.034875</td>\n      <td>...</td>\n      <td>-0.543020</td>\n      <td>-0.105807</td>\n      <td>0.171141</td>\n      <td>-0.105441</td>\n      <td>-0.231421</td>\n      <td>0.049828</td>\n      <td>-0.427197</td>\n      <td>-0.152171</td>\n      <td>-0.296015</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1828 rows × 51 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "vec_data = pd.concat([wordvec_df, data['label']], axis=1)\n",
    "vec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label = vec_data['label']\n",
    "attrs = vec_data.drop(columns=['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        attrs, label, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.7242888402625821\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.7177242888402626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=30, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}