{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            date                                               text president  \\\n",
       "0     2012-11-05  president obama tells the story of fired up re...     Obama   \n",
       "1     2012-11-06  election day is here confirm your polling plac...     Obama   \n",
       "2     2012-11-06  it’s election day this is your last chance to ...     Obama   \n",
       "3     2012-11-06  at the final rally of his final campaign last ...     Obama   \n",
       "4     2012-11-06  25 reasons that 25 people are voting for presi...     Obama   \n",
       "...          ...                                                ...       ...   \n",
       "12104 2020-06-16                                           true   …     Trump   \n",
       "12105 2020-06-16  a great woman her son is looking down from hea...     Trump   \n",
       "12106 2020-06-16  96% approval rating in the republican party th...     Trump   \n",
       "12107 2020-06-17  joe biden was a total failure in government he...     Trump   \n",
       "12108 2020-06-17    will be interviewed on   tonight at 900   enjoy     Trump   \n",
       "\n",
       "       label  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "12104     -1  \n",
       "12105     -1  \n",
       "12106     -1  \n",
       "12107     -1  \n",
       "12108     -1  \n",
       "\n",
       "[12109 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>text</th>\n      <th>president</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2012-11-05</td>\n      <td>president obama tells the story of fired up re...</td>\n      <td>Obama</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2012-11-06</td>\n      <td>election day is here confirm your polling plac...</td>\n      <td>Obama</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2012-11-06</td>\n      <td>it’s election day this is your last chance to ...</td>\n      <td>Obama</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2012-11-06</td>\n      <td>at the final rally of his final campaign last ...</td>\n      <td>Obama</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2012-11-06</td>\n      <td>25 reasons that 25 people are voting for presi...</td>\n      <td>Obama</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12104</th>\n      <td>2020-06-16</td>\n      <td>true   …</td>\n      <td>Trump</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>12105</th>\n      <td>2020-06-16</td>\n      <td>a great woman her son is looking down from hea...</td>\n      <td>Trump</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>12106</th>\n      <td>2020-06-16</td>\n      <td>96% approval rating in the republican party th...</td>\n      <td>Trump</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>12107</th>\n      <td>2020-06-17</td>\n      <td>joe biden was a total failure in government he...</td>\n      <td>Trump</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>12108</th>\n      <td>2020-06-17</td>\n      <td>will be interviewed on   tonight at 900   enjoy</td>\n      <td>Trump</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n<p>12109 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_excel('../Adatok/tisztitott_adat_nonconcat.xlsx', header=0,index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4272735, 5603020)"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "import gensim\n",
    "tokenized_tweet = data['text'].apply(lambda x: x.split())\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size=50, \n",
    "            window=5, \n",
    "            min_count=2,                                   \n",
    "            sg = 1,\n",
    "            hs = 0,\n",
    "            negative = 10, \n",
    "            workers= 32, \n",
    "            seed = 34\n",
    ")\n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(data['text']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('donald', 0.8830842971801758),\n",
       " ('j', 0.7494584321975708),\n",
       " ('frame', 0.7408056259155273),\n",
       " ('takedown', 0.6770744323730469),\n",
       " ('clinton', 0.6687959432601929),\n",
       " ('web', 0.6605334877967834),\n",
       " ('alleged', 0.6578599214553833),\n",
       " ('derangement', 0.6541644334793091),\n",
       " ('russians', 0.6494784951210022),\n",
       " ('colluded', 0.6414963006973267)]"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(positive=\"trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:  # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(12109, 50)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 50)) \n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 50)\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -0.095463  0.139151 -0.356881  0.091876  0.220736  0.448557  0.328842   \n",
       "1      0.015132  0.242092 -0.425064 -0.013810  0.167505  0.476959  0.281815   \n",
       "2     -0.125965  0.355396 -0.412121  0.078874  0.271823  0.535803  0.256289   \n",
       "3     -0.110323  0.147059 -0.403875  0.004709  0.215338  0.471057  0.335582   \n",
       "4     -0.162498  0.057550 -0.034758 -0.266167  0.264907  0.322853  0.422158   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "12104 -0.194127  0.328952 -0.638252 -0.238019  0.055882  0.103523  0.786820   \n",
       "12105  0.052348  0.085994 -0.105321 -0.145499  0.013745  0.398217  0.478521   \n",
       "12106 -0.179964  0.066921 -0.333907 -0.160486  0.172650  0.119559  0.356862   \n",
       "12107 -0.226806 -0.071491 -0.103074  0.058024  0.027885  0.230971  0.353388   \n",
       "12108 -0.504504  0.132016 -0.093686 -0.236592 -0.156115  0.558558  0.191424   \n",
       "\n",
       "              7         8         9  ...        41        42        43  \\\n",
       "0      0.331583  0.124128  0.283225  ...  0.023529 -0.111922 -0.233846   \n",
       "1      0.286692  0.018405  0.239796  ... -0.164217 -0.072055 -0.138283   \n",
       "2      0.379616  0.030027  0.075374  ... -0.138914  0.004187 -0.230320   \n",
       "3      0.341243  0.075930  0.255105  ...  0.013237 -0.052945 -0.256318   \n",
       "4      0.306710 -0.073539  0.108297  ...  0.109074  0.077723 -0.388956   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "12104 -0.166844  0.300118  0.410730  ...  0.220658  0.226717  0.112656   \n",
       "12105  0.338471  0.041541  0.113666  ... -0.262386 -0.047446 -0.002156   \n",
       "12106  0.629339  0.248059  0.356602  ... -0.107896  0.268739 -0.165073   \n",
       "12107  0.222439  0.202664  0.123746  ...  0.039351 -0.026419 -0.088226   \n",
       "12108  0.132511 -0.175697  0.039868  ... -0.146395 -0.115320 -0.098740   \n",
       "\n",
       "             44        45        46        47        48        49  label  \n",
       "0     -0.151305  0.192978  0.547401  0.419040 -0.049735 -0.486088      1  \n",
       "1     -0.128062  0.281515  0.612355  0.338310 -0.153471 -0.434059      1  \n",
       "2     -0.111103  0.345378  0.774623  0.320682 -0.135074 -0.384836      1  \n",
       "3     -0.137915  0.273358  0.531114  0.476591 -0.042361 -0.461569      1  \n",
       "4     -0.319276 -0.062973  0.512198  0.355353 -0.367513 -0.374694      1  \n",
       "...         ...       ...       ...       ...       ...       ...    ...  \n",
       "12104  0.154383  0.460603  0.776346  0.146590  0.019042 -0.080895     -1  \n",
       "12105 -0.225241  0.072200  0.634083  0.307751  0.039613 -0.218520     -1  \n",
       "12106 -0.056115  0.327610  0.052624  0.286369 -0.694977 -0.496204     -1  \n",
       "12107  0.044968  0.304888  0.470597  0.619008 -0.003580 -0.444521     -1  \n",
       "12108 -0.141079  0.507834  0.272255  0.436289 -0.463820 -0.440043     -1  \n",
       "\n",
       "[12109 rows x 51 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>41</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.095463</td>\n      <td>0.139151</td>\n      <td>-0.356881</td>\n      <td>0.091876</td>\n      <td>0.220736</td>\n      <td>0.448557</td>\n      <td>0.328842</td>\n      <td>0.331583</td>\n      <td>0.124128</td>\n      <td>0.283225</td>\n      <td>...</td>\n      <td>0.023529</td>\n      <td>-0.111922</td>\n      <td>-0.233846</td>\n      <td>-0.151305</td>\n      <td>0.192978</td>\n      <td>0.547401</td>\n      <td>0.419040</td>\n      <td>-0.049735</td>\n      <td>-0.486088</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.015132</td>\n      <td>0.242092</td>\n      <td>-0.425064</td>\n      <td>-0.013810</td>\n      <td>0.167505</td>\n      <td>0.476959</td>\n      <td>0.281815</td>\n      <td>0.286692</td>\n      <td>0.018405</td>\n      <td>0.239796</td>\n      <td>...</td>\n      <td>-0.164217</td>\n      <td>-0.072055</td>\n      <td>-0.138283</td>\n      <td>-0.128062</td>\n      <td>0.281515</td>\n      <td>0.612355</td>\n      <td>0.338310</td>\n      <td>-0.153471</td>\n      <td>-0.434059</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.125965</td>\n      <td>0.355396</td>\n      <td>-0.412121</td>\n      <td>0.078874</td>\n      <td>0.271823</td>\n      <td>0.535803</td>\n      <td>0.256289</td>\n      <td>0.379616</td>\n      <td>0.030027</td>\n      <td>0.075374</td>\n      <td>...</td>\n      <td>-0.138914</td>\n      <td>0.004187</td>\n      <td>-0.230320</td>\n      <td>-0.111103</td>\n      <td>0.345378</td>\n      <td>0.774623</td>\n      <td>0.320682</td>\n      <td>-0.135074</td>\n      <td>-0.384836</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.110323</td>\n      <td>0.147059</td>\n      <td>-0.403875</td>\n      <td>0.004709</td>\n      <td>0.215338</td>\n      <td>0.471057</td>\n      <td>0.335582</td>\n      <td>0.341243</td>\n      <td>0.075930</td>\n      <td>0.255105</td>\n      <td>...</td>\n      <td>0.013237</td>\n      <td>-0.052945</td>\n      <td>-0.256318</td>\n      <td>-0.137915</td>\n      <td>0.273358</td>\n      <td>0.531114</td>\n      <td>0.476591</td>\n      <td>-0.042361</td>\n      <td>-0.461569</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.162498</td>\n      <td>0.057550</td>\n      <td>-0.034758</td>\n      <td>-0.266167</td>\n      <td>0.264907</td>\n      <td>0.322853</td>\n      <td>0.422158</td>\n      <td>0.306710</td>\n      <td>-0.073539</td>\n      <td>0.108297</td>\n      <td>...</td>\n      <td>0.109074</td>\n      <td>0.077723</td>\n      <td>-0.388956</td>\n      <td>-0.319276</td>\n      <td>-0.062973</td>\n      <td>0.512198</td>\n      <td>0.355353</td>\n      <td>-0.367513</td>\n      <td>-0.374694</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12104</th>\n      <td>-0.194127</td>\n      <td>0.328952</td>\n      <td>-0.638252</td>\n      <td>-0.238019</td>\n      <td>0.055882</td>\n      <td>0.103523</td>\n      <td>0.786820</td>\n      <td>-0.166844</td>\n      <td>0.300118</td>\n      <td>0.410730</td>\n      <td>...</td>\n      <td>0.220658</td>\n      <td>0.226717</td>\n      <td>0.112656</td>\n      <td>0.154383</td>\n      <td>0.460603</td>\n      <td>0.776346</td>\n      <td>0.146590</td>\n      <td>0.019042</td>\n      <td>-0.080895</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>12105</th>\n      <td>0.052348</td>\n      <td>0.085994</td>\n      <td>-0.105321</td>\n      <td>-0.145499</td>\n      <td>0.013745</td>\n      <td>0.398217</td>\n      <td>0.478521</td>\n      <td>0.338471</td>\n      <td>0.041541</td>\n      <td>0.113666</td>\n      <td>...</td>\n      <td>-0.262386</td>\n      <td>-0.047446</td>\n      <td>-0.002156</td>\n      <td>-0.225241</td>\n      <td>0.072200</td>\n      <td>0.634083</td>\n      <td>0.307751</td>\n      <td>0.039613</td>\n      <td>-0.218520</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>12106</th>\n      <td>-0.179964</td>\n      <td>0.066921</td>\n      <td>-0.333907</td>\n      <td>-0.160486</td>\n      <td>0.172650</td>\n      <td>0.119559</td>\n      <td>0.356862</td>\n      <td>0.629339</td>\n      <td>0.248059</td>\n      <td>0.356602</td>\n      <td>...</td>\n      <td>-0.107896</td>\n      <td>0.268739</td>\n      <td>-0.165073</td>\n      <td>-0.056115</td>\n      <td>0.327610</td>\n      <td>0.052624</td>\n      <td>0.286369</td>\n      <td>-0.694977</td>\n      <td>-0.496204</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>12107</th>\n      <td>-0.226806</td>\n      <td>-0.071491</td>\n      <td>-0.103074</td>\n      <td>0.058024</td>\n      <td>0.027885</td>\n      <td>0.230971</td>\n      <td>0.353388</td>\n      <td>0.222439</td>\n      <td>0.202664</td>\n      <td>0.123746</td>\n      <td>...</td>\n      <td>0.039351</td>\n      <td>-0.026419</td>\n      <td>-0.088226</td>\n      <td>0.044968</td>\n      <td>0.304888</td>\n      <td>0.470597</td>\n      <td>0.619008</td>\n      <td>-0.003580</td>\n      <td>-0.444521</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>12108</th>\n      <td>-0.504504</td>\n      <td>0.132016</td>\n      <td>-0.093686</td>\n      <td>-0.236592</td>\n      <td>-0.156115</td>\n      <td>0.558558</td>\n      <td>0.191424</td>\n      <td>0.132511</td>\n      <td>-0.175697</td>\n      <td>0.039868</td>\n      <td>...</td>\n      <td>-0.146395</td>\n      <td>-0.115320</td>\n      <td>-0.098740</td>\n      <td>-0.141079</td>\n      <td>0.507834</td>\n      <td>0.272255</td>\n      <td>0.436289</td>\n      <td>-0.463820</td>\n      <td>-0.440043</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n<p>12109 rows × 51 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "vec_data = pd.concat([wordvec_df, data['label']], axis=1)\n",
    "vec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label = vec_data['label']\n",
    "attrs = vec_data.drop(columns=['label'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        attrs, label, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.5535006605019815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.5211360634081902\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=30, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}